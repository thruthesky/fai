# scripts/build_bin_dataset.py
# ============================================================================
# 바이너리 데이터셋 생성 스크립트
# ============================================================================
#
# 【개요】
# 텍스트 파일(samples.txt)을 토큰 ID 배열로 변환하여 바이너리 파일로 저장합니다.
# 이 과정은 GPT 학습의 전처리 단계로, 학습 속도를 크게 향상시킵니다.
#
# 【입력/출력】
# - 입력: data/samples.txt (학습용 텍스트)
#         data/tokenizer.json (BPE 토크나이저)
# - 출력: data/train.bin (학습 데이터, 99%)
#         data/val.bin (검증 데이터, 1%)
#
# ============================================================================
# 왜 바이너리 변환이 필요한가? (육하원칙)
# ============================================================================
#
# 【누가 (Who)】
# - GPT 모델 (PyTorch)이 학습할 때 사용합니다.
# - train_gpt.py 스크립트가 이 바이너리 파일을 읽습니다.
#
# 【언제 (When)】
# - 토크나이저 학습(train_tokenizer.py) 이후에 실행합니다.
# - GPT 학습(train_gpt.py) 이전에 반드시 실행해야 합니다.
# - 한 번만 실행하면 됩니다 (데이터가 바뀌지 않는 한).
#
# 【어디서 (Where)】
# - 로컬 파일 시스템에서 실행합니다.
# - train.bin과 val.bin은 data/ 폴더에 저장됩니다.
#
# 【무엇을 (What)】
# - 텍스트를 숫자(토큰 ID)로 변환합니다.
# - 예: "채용 정보" → [802, 712] (토큰 ID 배열)
# - NumPy uint16 배열로 저장합니다 (vocab_size < 65535이므로).
#
# 【어떻게 (How)】
# 1. tokenizer.json을 로드합니다.
# 2. samples.txt 전체를 읽어서 토큰화합니다.
# 3. 토큰 ID 배열을 NumPy 배열로 변환합니다.
# 4. 99:1 비율로 train/val을 분할합니다.
# 5. 바이너리 파일로 저장합니다.
#
# 【왜 (Why)】
# - 🚀 속도: 텍스트를 매번 토큰화하면 학습이 매우 느립니다.
#   바이너리는 숫자를 직접 읽으므로 I/O만 발생합니다.
#
# - 💾 메모리 효율: NumPy 메모리맵(mmap)으로 파일을 열면
#   전체 파일을 RAM에 로드하지 않고도 랜덤 접근이 가능합니다.
#
# - 🔄 재사용: 한 번 변환하면 여러 번 학습에 재사용할 수 있습니다.
#   하이퍼파라미터를 바꿔가며 실험할 때 전처리를 반복할 필요가 없습니다.
#
# ============================================================================
# 비교: 텍스트 vs 바이너리
# ============================================================================
#
# [텍스트 파일 방식 - 느림]
#   매 배치마다:
#   1. 텍스트 파일에서 문자열 읽기
#   2. 토크나이저로 토큰화 (문자열 → 토큰)
#   3. 토큰 → ID 변환
#   4. NumPy/Tensor 변환
#   5. GPU로 전송
#   → 2~4단계가 매번 반복되어 비효율적
#
# [바이너리 파일 방식 - 빠름]
#   전처리 (1회):
#   텍스트 → 토큰화 → ID 배열 → 바이너리 저장
#
#   학습 시 (매 배치):
#   1. 바이너리에서 숫자 직접 읽기 (mmap)
#   2. Tensor 변환
#   3. GPU로 전송
#   → 토큰화 단계 없이 바로 숫자를 읽음
#
# ============================================================================
# 데이터 형식
# ============================================================================
#
# - dtype: uint16 (2바이트, 0~65535 범위)
# - vocab_size가 24,000이므로 uint16이면 충분합니다.
# - uint32 대비 메모리 50% 절약
#
# 예시:
#   텍스트: "[QUESTION]\n이 연락처 정보를..."
#   토큰:   ['[', 'QUESTION', ']', '\n', '이', '연락처', ...]
#   ID:     [55, 747, 57, 123, 468, 709, ...]
#   저장:   바이너리 파일 (연속된 uint16 숫자)
#
# ============================================================================

import os
import numpy as np
from tokenizers import Tokenizer

# ============================================
# 설정
# ============================================
TEXT_PATH = "data/samples.txt"      # 입력 텍스트
TOK_PATH = "data/tokenizer.json"    # 토크나이저

TRAIN_OUT = "data/train.bin"        # 학습용 출력
VAL_OUT = "data/val.bin"            # 검증용 출력

VAL_RATIO = 0.01  # 검증 데이터 비율 (1%)

# ============================================
# 1단계: 토크나이저 로드
# ============================================
# tokenizer.json에서 학습된 BPE 토크나이저를 불러옵니다.
# 이 토크나이저는 텍스트를 토큰 ID로 변환하는 데 사용됩니다.
tokenizer = Tokenizer.from_file(TOK_PATH)
print(f"토크나이저 로드 완료: {TOK_PATH}")
print(f"어휘 크기: {tokenizer.get_vocab_size()}")

# ============================================
# 2단계: 텍스트 로드 및 토큰화
# ============================================
# samples.txt 전체를 메모리에 읽어옵니다.
# 대용량 파일의 경우 청크 단위로 처리하는 것이 좋지만,
# 현재 데이터 크기(~160KB)에서는 전체 로드가 효율적입니다.
with open(TEXT_PATH, "r", encoding="utf-8") as f:
    text = f.read()

print(f"텍스트 길이: {len(text):,} 문자")

# 토큰화: 텍스트 → 토큰 ID 리스트
# encode()는 텍스트를 토큰으로 분리하고, 각 토큰을 ID로 변환합니다.
# 예: "채용 정보" → ['채용', '정보'] → [802, 712]
ids = tokenizer.encode(text).ids
print(f"토큰 수: {len(ids):,}")

# NumPy 배열로 변환
# - uint16: 2바이트, 0~65535 범위
# - vocab_size(6,810)가 65535 이하이므로 uint16으로 충분
# - uint32(4바이트) 대비 메모리 50% 절약
arr = np.array(ids, dtype=np.uint16)

# ============================================
# 3단계: Train/Val 분할
# ============================================
# 데이터를 학습용(train)과 검증용(val)으로 분할합니다.
# - 학습용: 모델이 패턴을 학습하는 데 사용
# - 검증용: 학습 중 과적합 여부를 확인하는 데 사용
n = len(arr)
n_val = int(n * VAL_RATIO)

# n_val이 0인 경우 방지 (데이터가 매우 적을 때)
# arr[-0:]은 전체 배열을 반환하므로, 최소 1개의 검증 토큰 확보
if n_val == 0:
    n_val = min(100, n // 10)  # 최소 100개 또는 전체의 10%
    print(f"경고: 검증 데이터가 너무 적어 {n_val}개로 조정")

# 분할: 앞부분은 학습용, 뒷부분은 검증용
train = arr[:-n_val] if n_val > 0 else arr
val = arr[-n_val:] if n_val > 0 else np.array([], dtype=np.uint16)

# ============================================
# 4단계: 바이너리로 저장
# ============================================
# tofile(): NumPy 배열을 바이너리 파일로 직접 저장
# - 헤더 없이 raw 데이터만 저장
# - 읽을 때는 np.fromfile() 또는 np.memmap() 사용
train.tofile(TRAIN_OUT)
val.tofile(VAL_OUT)

print(f"\n학습 데이터: {len(train):,} 토큰 → {TRAIN_OUT}")
print(f"검증 데이터: {len(val):,} 토큰 → {VAL_OUT}")

# 파일 크기 출력 (참고용)
train_size = os.path.getsize(TRAIN_OUT)
val_size = os.path.getsize(VAL_OUT)
print(f"\n파일 크기:")
print(f"  {TRAIN_OUT}: {train_size:,} 바이트 ({train_size/1024:.1f} KB)")
print(f"  {VAL_OUT}: {val_size:,} 바이트 ({val_size/1024:.1f} KB)")