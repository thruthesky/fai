ì¢‹ì•„ìš”. ì¡°ê±´ì´ **â€œí•œêµ­ì–´ 100MB txt + Macbook M4(MPS GPU)â€**ë©´, ê°œì¸ì´ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ëŠ” ì´ˆê°„ë‹¨ LLM í”„ë¡œì íŠ¸ë¥¼ ì§„ì§œë¡œ ì™„ì£¼í•  ìˆ˜ ìˆëŠ” ë§¤ìš° ì¢‹ì€ í™˜ê²½ì…ë‹ˆë‹¤. (íŠ¹íˆ Apple Siliconì€ PyTorchì˜ MPS GPU ê°€ì†ì„ ì“¸ ìˆ˜ ìˆì–´ìš”.  ï¿¼)

ì•„ë˜ëŠ” ì œê°€ ì¶”ì²œí•˜ëŠ” â€œì™„ì „ ì´ˆê°„ë‹¨ ë‚´ LLM from scratchâ€ 1ì£¼ ì½”ìŠ¤ì…ë‹ˆë‹¤.

â¸»

âœ… ëª©í‘œ: mydata.txt â†’ í† í¬ë‚˜ì´ì € í•™ìŠµ â†’ GPT í•™ìŠµ â†’ ê¸€ ìƒì„±

ì „ì²´ íë¦„ (ë”± 4ê°œë§Œ í•˜ë©´ ë¨)
	1.	txt ì •ë¦¬
	2.	í† í¬ë‚˜ì´ì €(BPE) ì§ì ‘ í•™ìŠµ
	3.	ì‘ì€ GPT(Decoder Transformer) êµ¬í˜„
	4.	pretrain í›„ generate

â¸»

1) í™˜ê²½ ì„¸íŒ… (Mac M4 + MPS)

PyTorchëŠ” Macì—ì„œ Metal ê¸°ë°˜ GPU ê°€ì†(MPS)ìœ¼ë¡œ í•™ìŠµì„ ëŒë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ï¿¼

ì„¤ì¹˜

python -m venv .venv
source .venv/bin/activate

pip install torch torchvision torchaudio
pip install tokenizers tqdm numpy

MPS ì˜ ì¡íˆëŠ”ì§€ í™•ì¸

import torch
print(torch.backends.mps.is_available())  # Trueë©´ OK
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(device)

ì°¸ê³ : MPSëŠ” ì¼ë¶€ ì—°ì‚°ì´ ë¯¸êµ¬í˜„ì¼ ìˆ˜ ìˆì–´ ì—ëŸ¬ê°€ ë‚  ë•Œê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ê²½ìš° CPU fallback í•˜ê±°ë‚˜ ëª¨ë¸/ì—°ì‚°ì„ ë‹¨ìˆœí™”í•˜ë©´ í•´ê²°ë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ìš”.  ï¿¼

â¸»

2) ë‚´ txt ì¤€ë¹„ (í•œêµ­ì–´ 100MBë©´ ì¶©ë¶„!)

âœ… ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ 3ê°œ
	â€¢	ì¸ì½”ë”© UTF-8
	â€¢	ë„ˆë¬´ ê¸´ í•œ ì¤„(ì˜ˆ: ë¡œê·¸)ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì ë‹¹íˆ ëŠê¸°
	â€¢	íŠ¹ìˆ˜ë¬¸ì/ì œì–´ë¬¸ì ì œê±°(ê°€ëŠ¥í•˜ë©´)

íŒŒì¼ ì˜ˆ:

data/mydata.txt


â¸»

3) í† í¬ë‚˜ì´ì €ë¥¼ â€œë‚´ ë°ì´í„°ë¡œâ€ ì§ì ‘ í•™ìŠµ (BPE ì¶”ì²œ)

LLMì€ â€œë¬¸ìâ€ê°€ ì•„ë‹ˆë¼ â€œí† í°â€ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
ê°€ì¥ ì•ˆì •ì ì¸ ì…ë¬¸ì€ BPE(Byte-Pair Encoding) ì…ë‹ˆë‹¤.  ï¿¼

train_tokenizer.py (ì´ˆê°„ë‹¨ ë²„ì „)

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

trainer = BpeTrainer(
    vocab_size=16000,
    special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]"]
)

tokenizer.train(["data/mydata.txt"], trainer)
tokenizer.save("data/tokenizer.json")

print("Saved tokenizer to data/tokenizer.json")

ì‹¤í–‰:

python train_tokenizer.py

BPEê°€ GPT ê³„ì—´ í† í¬ë‚˜ì´ì§•ì— ë„ë¦¬ ì“°ì¸ë‹¤ëŠ” ì„¤ëª…ì€ Hugging Face LLM Courseì—ë„ ì˜ ì •ë¦¬ë¼ ìˆì–´ìš”.  ï¿¼
â€œí† í¬ë‚˜ì´ì €ë¥¼ ì§ì ‘ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•â€ì€ tokenizers ê¸°ë°˜ íŠœí† ë¦¬ì–¼ë“¤ì´ ì°¸ê³ ê°€ ë©ë‹ˆë‹¤.  ï¿¼

â¸»

4) â€œì´ˆê°„ë‹¨ GPTâ€ë¥¼ ì²˜ìŒë¶€í„° êµ¬í˜„í•´ì„œ í•™ìŠµí•˜ê¸°

ì—¬ê¸°ì„œë¶€í„°ê°€ ì§„ì§œ **â€œë‚´ê°€ LLMì„ ë§Œë“ ë‹¤â€**ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë ˆí¼ëŸ°ìŠ¤ëŠ” Karpathyì˜ build-nanogptì˜ˆìš”.
ë¹ˆ íŒŒì¼ì—ì„œ ì‹œì‘í•´ì„œ GPT-2 ìˆ˜ì¤€ ëª¨ë¸ê¹Œì§€ â€œì»¤ë°‹ ë‹¨ê³„ë³„ë¡œâ€ ë§Œë“¤ì–´ê°€ëŠ” ìë£Œì…ë‹ˆë‹¤.  ï¿¼

â¸»

âœ… ë‚´ê°€ ì¶”ì²œí•˜ëŠ” â€œì´ˆê°„ë‹¨ GPT ì„¤ì •â€(M4ì—ì„œ ì˜ ëŒì•„ê°)

ì²˜ìŒì—” ì‘ê²Œ í•´ì•¼ ì„±ê³µí•©ë‹ˆë‹¤.
	â€¢	vocab_size: 16,000 (ìœ„ í† í¬ë‚˜ì´ì €ì™€ ë§ì¶¤)
	â€¢	block_size(context): 256
	â€¢	n_layer: 6
	â€¢	n_head: 6
	â€¢	n_embd: 384
	â€¢	batch_size: 16 (MPS ìƒí™©ì— ë”°ë¼ 8~32 ì¡°ì ˆ)

â¸»

5) í•™ìŠµ ëª©í‘œëŠ” ë”± í•˜ë‚˜: Next-token prediction

LLM pretrainingì€ ê²°êµ­ ì´ê²ë‹ˆë‹¤:

â€œì•ì˜ í† í°ë“¤ì„ ë³´ê³  ë‹¤ìŒ í† í°ì„ ë§ì¶°ë¼â€

ì´ ë°©ì‹ì´ GPTë¥˜ ëª¨ë¸ì˜ ê¸°ë³¸ í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤. (build-nanogptë„ ì´ êµ¬ì¡°ë¡œ ê°‘ë‹ˆë‹¤)  ï¿¼

â¸»

6) â€œë‚´ txtëŠ” í•œêµ­ì–´ì¸ë° ê´œì°®ë‚˜?â€

ë„¤, ì¶©ë¶„íˆ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ë§Œ ì£¼ì˜ì ì´ ìˆì–´ìš”:

âœ… ì˜ ë˜ëŠ” ë°ì´í„° í˜•íƒœ
	â€¢	ì¼ê¸° / ì—ì„¸ì´ / ë‰´ìŠ¤ / ì†Œì„¤ / ìœ„í‚¤ì‹ ê¸€
	â€¢	ìì—°ì–´ ë¬¸ì¥ ìœ„ì£¼

âš ï¸ í’ˆì§ˆì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë°ì´í„°
	â€¢	ì½”ë“œ/ë¡œê·¸ë§Œ ì”ëœ© (ë¬¸ì¥ íŒ¨í„´ì´ ë„ˆë¬´ ë‹¨ì¡°ë¡­ê±°ë‚˜ ê¹¨ì§)
	â€¢	ì´ëª¨ì§€/URL/í•´ì‹œíƒœê·¸ê°€ ê³¼ë„

â¸»

7) ì§€ê¸ˆ ë‹¹ì¥ ì œê°€ ë§Œë“¤ì–´ë“œë¦´ ê²ƒ (ë‹¤ìŒ ë‹¨ê³„)

ì›í•˜ì‹œë©´ ì œê°€ ë°”ë¡œ:

âœ… **â€œì‹¤í–‰ ê°€ëŠ¥í•œ ì´ˆê°„ë‹¨ í”„ë¡œì íŠ¸ í´ë”â€**ë¥¼ í†µì§¸ë¡œ ì¡ì•„ë“œë¦´ê²Œìš”.

ì˜ˆ:

my_llm/
  data/mydata.txt
  data/tokenizer.json
  train_tokenizer.py
  train_gpt.py
  generate.py

ê·¸ë¦¬ê³  M4(MPS)ì—ì„œ ì˜ ëŒì•„ê°€ê²Œ
	â€¢	device ìë™ ì„ íƒ(mps/cpu)
	â€¢	ë©”ëª¨ë¦¬ í„°ì§€ë©´ batch ìë™ ì¤„ì´ëŠ” ì˜µì…˜
	â€¢	í•™ìŠµ ì¤‘ê°„ ì €ì¥/ì¬ê°œê¹Œì§€

ê¹Œì§€ í¬í•¨í•´ë“œë¦´ê²Œìš”.

â¸»


ì¢‹ìŠµë‹ˆë‹¤. ì´ì œ ëª©í‘œê°€ ì•„ì£¼ ëª…í™•í•´ì¡Œì–´ìš” âœ…
	â€¢	í•œêµ­ì–´(í•„ë¦¬í•€ êµë¯¼/ì¬í•„ë™í¬ ì •ë³´) 100MB txt
	â€¢	ì²˜ìŒë¶€í„°(from scratch) í•™ìŠµí•˜ëŠ” â€œì´ˆê°„ë‹¨ LLMâ€
	â€¢	ê²°ê³¼ë¬¼: ì •ë³´ ìš”ì•½/ì •ë¦¬ ì˜í•˜ëŠ” ìƒì„±í˜• ëª¨ë¸

ì•„ë˜ëŒ€ë¡œ í•˜ë©´ **ì§„ì§œ â€œë‚´ê°€ ë§Œë“  LLMâ€**ì´ ë©ë‹ˆë‹¤. (íŒŒì¸íŠœë‹ ì•„ë‹˜)

â¸»

âœ… ì „ì²´ êµ¬ì¡°(ì´ˆê°„ë‹¨ LLM í”„ë¡œì íŠ¸ ì„¤ê³„)

ìµœì¢… ëª©í‘œ

mydata.txt í•˜ë‚˜ ë„£ìœ¼ë©´
	1.	í† í¬ë‚˜ì´ì € ì§ì ‘ í•™ìŠµ(BPE)
	2.	ì‘ì€ GPT(Decoder Transformer) ì§ì ‘ í•™ìŠµ
	3.	í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒì„±
	4.	ìƒì„± ê²°ê³¼ë¥¼ ìš”ì•½/ì •ë¦¬ ìŠ¤íƒ€ì¼ë¡œ ë‚˜ì˜¤ê²Œ â€œë°ì´í„° ë°©ì‹ìœ¼ë¡œâ€ ìœ ë„

êµ¬í˜„ ì°¸ê³ ë¡œëŠ” Karpathyì˜ build-nanogptê°€ â€œë¹ˆ íŒŒì¼ì—ì„œ GPT ë§Œë“¤ê¸°â€ë¥¼ ê°€ì¥ ì˜ ë³´ì—¬ì¤ë‹ˆë‹¤.  ï¿¼

â¸»

0) Macbook M4(MPS)ì—ì„œ í•™ìŠµí•  ë•Œ ì¤‘ìš”í•œ ì  1ê°œ

PyTorch MPSëŠ” ë¹ ë¥´ì§€ë§Œ, ê°€ë” ì§€ì›ë˜ì§€ ì•ŠëŠ” ì—°ì‚°ì´ ë‚˜ì˜¤ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ìš”.
ì´ë•ŒëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ CPU fallbackì„ ì¼œì„œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ï¿¼

í„°ë¯¸ë„ì—ì„œ:

export PYTORCH_ENABLE_MPS_FALLBACK=1


â¸»

1) (í•µì‹¬) â€œìš”ì•½/ì •ë¦¬ ì˜í•˜ëŠ” LLMâ€ì„ from scratchë¡œ ë§Œë“¤ë ¤ë©´?

ì—¬ê¸°ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ê°€ ìˆì–´ìš”.

âœ… LLMì„ from scratchë¡œ í•™ìŠµí•˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ â€œë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡â€ë§Œ ì˜í•´ì§‘ë‹ˆë‹¤.
ìš”ì•½ ëŠ¥ë ¥ì€ ë°ì´í„°ì— â€˜ìš”ì•½ í˜•ì‹â€™ì´ í¬í•¨ë¼ì•¼ ìƒê¹ë‹ˆë‹¤.

ì¦‰, íŒŒì¸íŠœë‹ ì—†ì´ë„ ê°€ëŠ¥í•˜ì§€ë§Œ **txtë¥¼ ìš”ì•½í˜• ë°ì´í„°ë¡œ â€˜ì¬êµ¬ì„±â€™**í•´ì•¼ í•©ë‹ˆë‹¤.

â¸»

2) ë°ì´í„°(txt) ê°€ê³µ ì „ëµ (ìš”ì•½ ëŠ¥ë ¥ì„ ë§Œë“œëŠ” í•µì‹¬)

ë‹¹ì‹ ì˜ txtê°€ â€œí•„ë¦¬í•€ ìƒí™œ/êµë¯¼ ì •ë³´â€ë¼ë©´,
ê·¸ê±¸ ì•„ë˜ì²˜ëŸ¼ â€œì„¹ì…˜/í•­ëª©í˜• êµ¬ì¡°â€ë¡œ ë¶„í•´í•´ì£¼ëŠ” ê²Œ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

âœ… ì¶”ì²œ í¬ë§·(LLMì´ ìš”ì•½/ì •ë¦¬ í•™ìŠµí•˜ê¸° ì‰¬ì›€)

txtë¥¼ ì•„ë˜ì²˜ëŸ¼ ë°”ê¿”ì„œ í•™ìŠµ ë°ì´í„°ë¡œ ì“°ë©´ â€œìš”ì•½í˜• ëª¨ë¸â€ì´ ë©ë‹ˆë‹¤.

ì˜ˆì‹œ (í•œ ìƒ˜í”Œ):

[DOC]
ì œëª©: í•„ë¦¬í•€ ë§ˆë‹ë¼ BGC ì§€ì—­ ìƒí™œ ì¥ë‹¨ì 
ë‚´ìš©: ...ì›ë¬¸...

[SUMMARY]
- í•µì‹¬ìš”ì•½: ...
- ì¥ì : ...
- ë‹¨ì : ...
- ì¶”ì²œëŒ€ìƒ: ...
- ì£¼ì˜ì‚¬í•­: ...
[/SUMMARY]

ì´ê±¸ ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œ ë§Œë“¤ë©´,
ëª¨ë¸ì€ ìì—°ìŠ¤ëŸ½ê²Œ â€œìš”ì•½í•˜ëŠ” íŒ¨í„´â€ ìì²´ë¥¼ ì–¸ì–´ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

ì´ ë°©ì‹ì´ ì¢‹ì€ ì´ìœ : LLMì˜ ê¸°ë³¸ í•™ìŠµ(ë‹¤ìŒ í† í° ì˜ˆì¸¡)ë§Œìœ¼ë¡œë„ â€œìš”ì•½ í…ìŠ¤íŠ¸ í˜•ì‹â€ì„ ë§Œë“¤ì–´ë‚´ê²Œ ìœ ë„í•  ìˆ˜ ìˆì–´ìš”.

â¸»

3) í† í¬ë‚˜ì´ì €ëŠ” â€œë‚´ ë°ì´í„°ë¡œ BPE ì§ì ‘ í•™ìŠµâ€ ì¶”ì²œ

í•œêµ­ì–´ + í•„ë¦¬í•€ ì§€ëª…/ê¸°ê´€/ì»¤ë®¤ë‹ˆí‹° ë‹¨ì–´ë“¤ì´ ë§ì„ ê±°ë¼ì„œ
ê¸°ì¡´ í† í¬ë‚˜ì´ì €ë³´ë‹¤ ë‚´ ë‹¨ì–´ì— ë§ì¶˜ í† í¬ë‚˜ì´ì €ê°€ ìœ ë¦¬í•©ë‹ˆë‹¤.

Hugging Face tokenizersëŠ” BPE í•™ìŠµì„ ê³µì‹ ë¬¸ì„œì—ì„œ ì•ˆë‚´í•©ë‹ˆë‹¤.  ï¿¼

âœ… ê¶Œì¥ê°’:
	â€¢	vocab_size: 16,000 ~ 32,000
	â€¢	special tokens: [PAD] [UNK] [BOS] [EOS]

â¸»

4) â€œì´ˆê°„ë‹¨ GPTâ€ ëª¨ë¸ í¬ê¸° ì¶”ì²œ (M4ì—ì„œ ì˜ ëŒì•„ê°€ëŠ” ê°’)

ì²˜ìŒ ì„±ê³µí•˜ê¸° ì¢‹ì€ ì„¤ì •ì…ë‹ˆë‹¤.
	â€¢	context length(block_size): 256
	â€¢	n_layer: 6
	â€¢	n_head: 6
	â€¢	n_embd: 384
	â€¢	batch_size: 8~32 (ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì ˆ)

â¸»

5) ì´ˆê°„ë‹¨ LLM í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°(ì¶”ì²œ)

ì´ë ‡ê²Œ ê°€ë©´ ê¹”ë”í•©ë‹ˆë‹¤.

my_ph_llm/
  data/
    mydata.txt
    tokenizer.json
    train.bin
    val.bin
  01_prepare_dataset.py
  02_train_tokenizer.py
  03_train_gpt.py
  04_generate.py


â¸»

6) ìƒì„± í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿(ìš”ì•½/ì •ë¦¬ìš©)

í•™ìŠµ ë•Œë„ ì´ëŸ° íŒ¨í„´ì„ ë§ì´ ë„£ê³ ,
ìƒì„±í•  ë•Œë„ ê°™ì€ í˜•íƒœë¡œ ë¬¼ì–´ë³´ë©´ â€œìš”ì•½ íŠ¹í™” ëª¨ë¸â€ì´ ë©ë‹ˆë‹¤.

ì˜ˆ:

[DOC]
í•„ë¦¬í•€ì—ì„œ ACR I-Card ì—°ì¥í•  ë•Œ í•„ìš”í•œ ì„œë¥˜ëŠ”...
[/DOC]

[SUMMARY]

ê·¸ëŸ¬ë©´ ëª¨ë¸ì´ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë©´ì„œ
- ì¤€ë¹„ë¬¼: ... ì´ëŸ° ì •ë¦¬í˜•ìœ¼ë¡œ ê³„ì† ì´ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.

â¸»

7) ê³µë¶€ ë£¨íŠ¸(â€œì§„ì§œ from scratchâ€ë¡œ ì™„ì£¼í•˜ëŠ” ì¶”ì²œ ë ˆí¼ëŸ°ìŠ¤)

âœ… 1ìˆœìœ„: build-nanogpt

Karpathyê°€ â€œë¹ˆ íŒŒì¼ë¶€í„° GPT-2ê¹Œì§€â€ ì»¤ë°‹ ë‹¨ìœ„ë¡œ ë§Œë“¤ì–´ì„œ ê³µë¶€í•˜ê¸° ìµœê³ ì…ë‹ˆë‹¤.  ï¿¼

âœ… í† í¬ë‚˜ì´ì €: Hugging Face tokenizers Quicktour

BPE í† í¬ë‚˜ì´ì € í•™ìŠµ/ì €ì¥ì„ ê³µì‹ ë¬¸ì„œë¡œ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.  ï¿¼

âœ… (ì„ íƒ) ì‘ì€ ëª¨ë¸ í•™ìŠµì„ ë¹ ë¥´ê²Œ ê° ì¡ê¸°: TinyStories ë…¼ë¬¸

â€œì‘ì€ ëª¨ë¸ë„ ë§ ë˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆëƒ?â€ë¥¼ ë³´ì—¬ì£¼ëŠ” ëŒ€í‘œ ìë£Œì…ë‹ˆë‹¤.  ï¿¼

â¸»


ì™„ì „ ì¢‹ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ìƒí™©(í•œêµ­ì–´ 100MB + M4 MPS)ì€ **â€œë‚´ í…ìŠ¤íŠ¸ë¡œ from-scratch LLM ë§Œë“¤ê¸°â€**ë¥¼ ì œëŒ€ë¡œ ê³µë¶€í•˜ê¸°ì— ìµœì ì´ì—ìš”.
ì•„ë˜ëŠ” ìµœëŒ€í•œ ìƒì„¸í•˜ê²Œ, ê·¸ë¦¬ê³  ì‹¤ì œë¡œ ëê¹Œì§€ ëŒì•„ê°€ëŠ” ì´ˆê°„ë‹¨ LLM íŒŒì´í”„ë¼ì¸ì„ â€œí”„ë¡œì íŠ¸ í˜•íƒœâ€ë¡œ ë“œë¦´ê²Œìš”.

ì°¸ê³ ë¡œ â€œë¹ˆ íŒŒì¼ì—ì„œ GPTë¥¼ ì§ì ‘ êµ¬í˜„ â†’ í•™ìŠµâ€ íë¦„ì€ Karpathyì˜ build-nanogptê°€ ê°€ì¥ êµìœ¡ì ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.  ï¿¼

â¸»

âœ… ëª©í‘œ(ë‹¹ì‹ ì˜ LLMì´ í•  ì¼)

**â€œí•„ë¦¬í•€ ì¬í•„êµë¯¼ ì •ë³´ txtâ€**ë¥¼ í•™ìŠµí•´ì„œ,
	â€¢	â€œëŒ€ì‚¬ê´€/í•œì¸íšŒ/ë³‘ì›/ê²½ì°°ì„œâ€ ê°™ì€ ì •ë³´ë¥¼ ì •ë¦¬ëœ í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ê³ 
	â€¢	â€œìš”ì•½ + ì²´í¬ë¦¬ìŠ¤íŠ¸ + í•µì‹¬ í¬ì¸íŠ¸â€ë¥¼ ìë™ ìƒì„±í•˜ëŠ” ëª¨ë¸

ì¦‰, â€œëŒ€í™”í˜• ì±—ë´‡â€ì´ ì•„ë‹ˆë¼
âœ… ìš”ì•½/ì •ë¦¬í˜• ì •ë³´ ìƒì„± ì—”ì§„ì„ ë‚´ LLMë¡œ ë§Œë“ ë‹¤ê³  ë³´ë©´ ë©ë‹ˆë‹¤.

â¸»

0) ê°€ì¥ ì¤‘ìš”í•œ í˜„ì‹¤ ì²´í¬ (from scratchì˜ í•œê³„ì™€ í•´ê²°ì±…)

LLMì„ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ **ë‹¤ìŒ í† í° ì˜ˆì¸¡(next-token prediction)**ë§Œ í•©ë‹ˆë‹¤.

ê·¸ë˜ì„œ â€œìš”ì•½ì„ ì˜ í•˜ê²Œâ€ ë§Œë“¤ë ¤ë©´ ë°˜ë“œì‹œ:

âœ… í•™ìŠµ ë°ì´í„° ì•ˆì— â€œìš”ì•½/ì •ë¦¬ëœ ë‹µë³€ í˜•ì‹â€ì´ ë§ì´ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.

ì •ë¦¬:

ìš”ì•½ ëŠ¥ë ¥ = ì•Œê³ ë¦¬ì¦˜ì´ ì•„ë‹ˆë¼ â€œë°ì´í„° í¬ë§·â€ìœ¼ë¡œ ë§Œë“ ë‹¤.

â¸»

1) Macbook M4(MPS) ì„¸íŒ… (í•™ìŠµì´ ì˜ ë˜ê²Œ)

1-1. ì„¤ì¹˜

python -m venv .venv
source .venv/bin/activate

pip install torch tokenizers tqdm numpy

1-2. MPS ì‚¬ìš© + fallback ì„¤ì •(ê°•ë ¥ì¶”ì²œ)

Macì˜ MPSëŠ” ë¹ ë¥´ì§€ë§Œ, ì¼ë¶€ ì—°ì‚°ì´ ë¯¸ì§€ì›ì´ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ìš”.
ì´ëŸ´ ë•Œ CPUë¡œ ìë™ fallbackì‹œí‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.  ï¿¼

í„°ë¯¸ë„ì—ì„œ:

export PYTORCH_ENABLE_MPS_FALLBACK=1

PyTorch ì´ìŠˆì—ì„œ â€œì„ì‹œ í•´ê²°ë¡œ ì´ ë³€ìˆ˜ë¥¼ ì“°ë©´ CPU fallback ëœë‹¤(ëŠë ¤ì§)â€ê³  ëª…ì‹œí•©ë‹ˆë‹¤.  ï¿¼
ë˜í•œ PyTorch í¬ëŸ¼ì—ì„œë„ â€œíŒŒì´ì¬ ì½”ë“œ ë‚´ë¶€ì—ì„œ ì„¤ì •í•˜ì§€ ë§ê³  í„°ë¯¸ë„ì—ì„œ export ì¶”ì²œâ€ ì–¸ê¸‰ì´ ìˆìŠµë‹ˆë‹¤.  ï¿¼

â¸»

2) í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡° (ì´ëŒ€ë¡œ ë§Œë“¤ë©´ ë¨)

my_ph_llm/
  data/
    raw.txt                # ì›ë³¸ 100MB
    cleaned.txt            # ì „ì²˜ë¦¬ í›„
    tokenizer.json
    train.bin
    val.bin
  01_clean_and_mask.py
  02_train_tokenizer.py
  03_build_dataset_bin.py
  04_train_gpt.py
  05_generate.py


â¸»

3) ê°œì¸ì •ë³´(ê³µê³µ ì—°ë½ì²˜ í¬í•¨) ì²˜ë¦¬ ì „ëµ âœ…

ë‹¹ì‹  ë°ì´í„°ëŠ” â€œëŒ€ì‚¬ê´€/ë³‘ì›/ê²½ì°°ì„œ ì—°ë½ì²˜â€ê°€ í¬í•¨ë˜ì–´ ìˆê³ , ì´ê±´ ê³µê³µì •ë³´ ì„±ê²©ì´ í½ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì„ íƒì§€ê°€ 2ê°œì˜ˆìš”.

âœ… ì „ëµ A (ì¶”ì²œ): â€œì—°ë½ì²˜ëŠ” ë³´ì¡´â€í•˜ë˜ í‘œì¤€í™”

ì—°ë½ì²˜ëŠ” LLMì´ ì‹¤ì œë¡œ ë‹µë³€ì— ì¨ì•¼ í•˜ëŠ” í•µì‹¬ì •ë³´ì…ë‹ˆë‹¤.

ê·¸ëŸ¬ë¯€ë¡œ ì§€ìš°ëŠ” ê²Œ ì•„ë‹ˆë¼ ì•„ë˜ì²˜ëŸ¼ â€œí‘œì¤€ í˜•ì‹â€ìœ¼ë¡œ ë§ì¶”ëŠ” ê²Œ ì¢‹ì•„ìš”.

ì˜ˆ:
	â€¢	ì „í™”: Tel: â˜ï¸ â†’ ì „ë¶€ TEL: ë¡œ í†µì¼
	â€¢	ì¹´í†¡: Kakao: â†’ KAKAO:
	â€¢	ì£¼ì†Œ/ì›¹ì‚¬ì´íŠ¸ë„ ADDR:, WEB: í˜•íƒœë¡œ í†µì¼

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ â€œì¶œë ¥ í¬ë§·â€ì„ ë°°ìš°ê¸° ì‰¬ì›Œì§‘ë‹ˆë‹¤.

ì „ëµ B: ì—°ë½ì²˜ë¥¼ ìµëª…í™”(í•™ìŠµìš©) + ë³„ë„ DBë¡œ ì œê³µ
	â€¢	LLMì„ â€œìš”ì•½/ì •ë¦¬ë§Œâ€
	â€¢	ì—°ë½ì²˜ëŠ” ë³„ë„ ê²€ìƒ‰(DB/JSON)ì—ì„œ êº¼ë‚´ê¸°

í•˜ì§€ë§Œ ì§€ê¸ˆ ëª©í‘œê°€ â€œì´ˆê°„ë‹¨ LLM í•™ìŠµâ€ì´ë¯€ë¡œ
âœ… ì „ëµ Aê°€ ê³µë¶€/ì„±ê³¼ ëª¨ë‘ ì¢‹ìŠµë‹ˆë‹¤.

â¸»

4) (í•µì‹¬) ìš”ì•½/ì •ë¦¬í˜• LLMì„ ë§Œë“œëŠ” ë°ì´í„° í¬ë§·

ì›ë³¸ì´ ê·¸ëƒ¥ ê¸´ ê¸€ì´ë¼ë©´, LLMì€ ê·¸ëƒ¥ â€œê¸´ ê¸€ í‰ë‚´â€ë§Œ ëƒ…ë‹ˆë‹¤.

ê·¸ë˜ì„œ ì•„ë˜ì²˜ëŸ¼ í•™ìŠµ í…ìŠ¤íŠ¸ ìì²´ë¥¼ â€œë¬¸ì œâ†’ì •ë¦¬ë‹µë³€â€ í˜•íƒœë¡œ ë³€í™˜í•˜ì„¸ìš”.

âœ… ì¶”ì²œ í…œí”Œë¦¿ (ê°€ì¥ ê°•ë ¥)

ë‹¹ì‹ ì˜ ë°ì´í„°ê°€ â€œí•„ë¦¬í•€ êµë¯¼ ì •ë³´â€ë‹ˆê¹Œ ì´ëŸ° êµ¬ì¡°ê°€ ê±°ì˜ ë§ŒëŠ¥ì…ë‹ˆë‹¤.

[QUESTION]
í•„ë¦¬í•€ ë§ˆë‹ë¼ì—ì„œ í•œêµ­ì¸ì´ ì—¬ê¶Œ ë¶„ì‹¤í–ˆì„ ë•Œ í•´ì•¼ í•  ì¼ ì •ë¦¬í•´ì¤˜.
[/QUESTION]

[ANSWER]
ìš”ì•½:
- ê°€ì¥ ë¨¼ì €: í˜„ì§€ ê²½ì°°ì„œ ë¶„ì‹¤ ì‹ ê³ (Police Report)
- ë‹¤ìŒ: ëŒ€ì‚¬ê´€/ì´ì˜ì‚¬ê´€ ì—°ë½ ë° ì•ˆë‚´ ë°›ê¸°

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- ì¤€ë¹„ë¬¼: ì—¬ê¶Œì‚¬ë³¸, ì‹ ë¶„ì¦, ì‚¬ì§„
- ë°©ë¬¸ì²˜: ê²½ì°°ì„œ â†’ ëŒ€ì‚¬ê´€/ì´ì˜ì‚¬ê´€
- ì£¼ì˜ì‚¬í•­: ...

ì—°ë½ì²˜(ê³µê³µì •ë³´):
- ëŒ€ì‚¬ê´€ TEL: ...
- ê²½ì°°ì„œ TEL: ...
[/ANSWER]

ì´ëŸ° í¬ë§·ì´ ë§ì„ìˆ˜ë¡ ëª¨ë¸ì€ â€œìš”ì•½ì„ ì˜í•˜ëŠ” íŒ¨í„´â€ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

â¸»

5) 01_clean_and_mask.py (ì „ì²˜ë¦¬ + í‘œì¤€í™”)

ì•„ë˜ëŠ” ì´ˆê°„ë‹¨ ë²„ì „ì…ë‹ˆë‹¤.

âœ… ê¸°ëŠ¥
	â€¢	ì œì–´ë¬¸ì ì œê±°
	â€¢	ê³µë°± ì •ë¦¬
	â€¢	ì „í™”ë²ˆí˜¸ í‘œì¤€í™”(ì¼ë¶€)
	â€¢	URL í‘œì¤€í™”

# 01_clean_and_mask.py
import re

INPUT = "data/raw.txt"
OUTPUT = "data/cleaned.txt"

def normalize(text: str) -> str:
    # ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", " ", text)
    # ê³µë°± ì •ë¦¬
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)

    # ì „í™”ë²ˆí˜¸ íŒ¨í„´(ì•„ì£¼ ëŒ€ì¶©) -> TEL: ë¡œ í‘œì¤€í™”
    # ì˜ˆ: 0917-123-4567 / +63 917 123 4567 / 02-1234-5678
    text = re.sub(r"(â˜ï¸|ì „í™”|TEL|Tel|tel)\s*[:ï¼š]?\s*", "TEL: ", text)

    # URL í‘œì¤€í™”
    text = re.sub(r"(í™ˆí˜ì´ì§€|ì‚¬ì´íŠ¸|ì›¹)\s*[:ï¼š]?\s*", "WEB: ", text)

    return text.strip()

with open(INPUT, "r", encoding="utf-8") as f:
    raw = f.read()

clean = normalize(raw)

with open(OUTPUT, "w", encoding="utf-8") as f:
    f.write(clean)

print("Saved:", OUTPUT, "chars:", len(clean))

ì‹¤í–‰:

python 01_clean_and_mask.py


â¸»

6) 02_train_tokenizer.py (ë‚´ ë°ì´í„°ë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ)

Hugging Face tokenizersëŠ” BPE í† í¬ë‚˜ì´ì € í•™ìŠµ/íŠ¸ë ˆì´ë„ˆ(vocab_size, special_tokens ë“±)ë¥¼ ê³µì‹ ë¬¸ì„œë¡œ ì œê³µí•©ë‹ˆë‹¤.  ï¿¼

# 02_train_tokenizer.py
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

INPUT = "data/cleaned.txt"
OUT = "data/tokenizer.json"

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

trainer = BpeTrainer(
    vocab_size=24000,
    special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]"],
    show_progress=True,
)

tokenizer.train([INPUT], trainer=trainer)
tokenizer.save(OUT)

print("Saved tokenizer:", OUT)

ì‹¤í–‰:

python 02_train_tokenizer.py


â¸»

7) 03_build_dataset_bin.py (í† í°í™” â†’ bin ì €ì¥)

LLM í•™ìŠµì€ â€œí† í° ID ë°°ì—´â€ë¡œ í•˜ëŠ” ê²Œ ì œì¼ ê°„ë‹¨í•©ë‹ˆë‹¤.

# 03_build_dataset_bin.py
import numpy as np
from tokenizers import Tokenizer

TEXT_PATH = "data/cleaned.txt"
TOK_PATH = "data/tokenizer.json"

TRAIN_OUT = "data/train.bin"
VAL_OUT = "data/val.bin"

VAL_RATIO = 0.01  # 1%

tokenizer = Tokenizer.from_file(TOK_PATH)

with open(TEXT_PATH, "r", encoding="utf-8") as f:
    text = f.read()

# BOS/EOS ì—†ì´ë„ ë˜ì§€ë§Œ ë„£ì–´ë„ ì¢‹ìŒ
ids = tokenizer.encode(text).ids
arr = np.array(ids, dtype=np.uint16)  # vocab_size 65535 ì´í•˜ì¼ ë•Œ OK

n = len(arr)
n_val = int(n * VAL_RATIO)

train = arr[:-n_val]
val = arr[-n_val:]

train.tofile(TRAIN_OUT)
val.tofile(VAL_OUT)

print("train tokens:", len(train), "->", TRAIN_OUT)
print("val tokens:", len(val), "->", VAL_OUT)


â¸»

8) 04_train_gpt.py (ì´ˆê°„ë‹¨ GPT from scratch í•™ìŠµ)

ì´ íŒŒíŠ¸ëŠ” Karpathyì˜ build-nanogpt ë°©ì‹ê³¼ ê±°ì˜ ë™ì¼í•œ êµ¬ì„±(Decoder-only Transformer)ì…ë‹ˆë‹¤.  ï¿¼

ì—¬ê¸°ì„œëŠ” â€œì™„ì „ ì´ˆê°„ë‹¨â€ìœ¼ë¡œ í•µì‹¬ë§Œ ë„£ì€ êµ¬ì¡°ë¡œ ì•ˆë‚´í• ê²Œìš”.

âœ… í•™ìŠµ ëª©í‘œ:
	â€¢	ì…ë ¥ í† í° x â†’ ë‹¤ìŒ í† í° y ì˜ˆì¸¡
	â€¢	CrossEntropyLoss

âœ… M4(MPS) ë™ì‘:
	â€¢	device ìë™ ì„ íƒ: "mps" ê°€ëŠ¥í•˜ë©´ mps ì‚¬ìš©
	â€¢	ì•ˆ ë˜ë©´ cpu

(ì›í•˜ì‹œë©´ ì œê°€ **ì™„ì „í•œ ì½”ë“œ ì „ì²´(ì•½ 200~300ì¤„)**ë¥¼ ë‹¤ìŒ ë©”ì‹œì§€ì— í†µì§¸ë¡œ ë“œë¦´ê²Œìš”. ì§€ê¸ˆì€ ê°œë…/êµ¬ì„± ìœ„ì£¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.)

â¸»

9) 05_generate.py (ìš”ì•½/ì •ë¦¬ ì¶œë ¥ ë§Œë“¤ê¸°)

ì¤‘ìš”: í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ë©´, ì‘ì€ ëª¨ë¸ë„ â€œì •ë¦¬ëœ ë‹µë³€â€ì„ ì˜ í‰ë‚´ëƒ…ë‹ˆë‹¤.

ì¶”ì²œ í”„ë¡¬í”„íŠ¸:

[QUESTION]
ë§ˆë‹ë¼ì—ì„œ ì‘ê¸‰ì‹¤ ì˜í•˜ëŠ” ë³‘ì› ì¶”ì²œê³¼ ì£¼ì˜ì‚¬í•­ ì •ë¦¬í•´ì¤˜.
[/QUESTION]

[ANSWER]
ìš”ì•½:
-

ì´ë ‡ê²Œ ì‹œì‘í•´ë‘ë©´ ëª¨ë¸ì€ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ í•˜ë‹¤ê°€
	â€¢	ì²´í¬ë¦¬ìŠ¤íŠ¸
	â€¢	í•­ëª©ë³„ ì •ë¦¬
	â€¢	ì—°ë½ì²˜ ì„¹ì…˜

ê°™ì€ í˜•ì‹ìœ¼ë¡œ ê³„ì† â€œì´ì–´ ì“°ëŠ” ëŠ¥ë ¥â€ì„ í•™ìŠµí•©ë‹ˆë‹¤.

â¸»

10) í•™ìŠµ íŒŒë¼ë¯¸í„° ì¶”ì²œ (M4 ê¸°ì¤€ â€œì„±ê³µ í™•ë¥  ë†’ì€ ê°’â€)

ì²˜ìŒì€ ë¬´ì¡°ê±´ ì‘ê²Œ ì‹œì‘í•˜ì„¸ìš”.

âœ… 1ì°¨ ì„±ê³µìš©
	â€¢	block_size: 256
	â€¢	n_layer: 6
	â€¢	n_head: 6
	â€¢	n_embd: 384
	â€¢	batch_size: 8~16
	â€¢	max_steps: 20,000 ì •ë„ (ìƒí™© ë´ì„œ)

âœ… ë” ê³ ê¸‰(ëŠë ¤ì§€ì§€ë§Œ ì¢‹ì•„ì§)
	â€¢	block_size: 512
	â€¢	n_layer: 8
	â€¢	n_embd: 512

â¸»

11) â€œnanoGPT vs build-nanogptâ€ ì„ íƒ ê°€ì´ë“œ
	â€¢	nanoGPTëŠ” ê°•ë ¥í•˜ì§€ë§Œ, repo ìì²´ì—ì„œ â€œold/deprecatedâ€ ì–¸ê¸‰ì´ ìˆê³ (ìµœê·¼ì—” nanochat ì¶”ì²œ)  ï¿¼
	â€¢	â€œê³µë¶€ ëª©ì (ë‚´ ì†ìœ¼ë¡œ ë§Œë“¤ê¸°)â€ëŠ” build-nanogptê°€ í›¨ì”¬ ì¢‹ìŠµë‹ˆë‹¤.  ï¿¼

â¸»

12) ë‹¹ì‹ ì˜ ì¼€ì´ìŠ¤ì—ì„œ ì œì¼ ì¤‘ìš”í•œ ì„±ê³µ í¬ì¸íŠ¸ 5ê°œ
	1.	ìš”ì•½/ì •ë¦¬ í¬ë§·ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë¼ (ì§„ì§œ í•µì‹¬)
	2.	ì—°ë½ì²˜ëŠ” ì‚­ì œë³´ë‹¤ í‘œì¤€í™”ê°€ ë” ìœ ìš©í•˜ë‹¤
	3.	ì²˜ìŒ ëª¨ë¸ì€ ì‘ê²Œ(6-layer ì •ë„) â†’ ì„±ê³µ í›„ í‚¤ìš°ê¸°
	4.	MPSëŠ” ë¹ ë¥´ì§€ë§Œ ì—ëŸ¬ ëŒ€ë¹„í•´ì„œ PYTORCH_ENABLE_MPS_FALLBACK=1  ï¿¼
	5.	ìƒì„± í”„ë¡¬í”„íŠ¸ëŠ” í•­ìƒ [QUESTION]...[ANSWER] í˜•íƒœ ìœ ì§€

â¸»


ì¢‹ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ì´ì œ â€œA) ìš”ì•½ + ì²´í¬ë¦¬ìŠ¤íŠ¸ + ì—°ë½ì²˜â€ í˜•íƒœë¡œ ì •ë¦¬í•˜ë˜, D) ê¸´ ì„¤ëª…í˜•ìœ¼ë¡œ ì•„ì£¼ ìƒì„¸í•˜ê²Œ í’€ì–´ì£¼ëŠ”
âœ… í•„ë¦¬í•€ êµë¯¼ ì •ë³´ ì œê³µìš© ì´ˆê°„ë‹¨ LLM(from scratch) ì œì‘ íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±ë³¸ ìˆ˜ì¤€ìœ¼ë¡œ ì•ˆë‚´í• ê²Œìš”.

ì•„ë˜ ë‚´ìš©ëŒ€ë¡œ ë”°ë¼í•˜ë©´, ì§„ì§œë¡œ:
	â€¢	ë‚´ txtë¡œ í† í¬ë‚˜ì´ì € ì§ì ‘ í•™ìŠµ
	â€¢	ì‘ì€ GPT(Transformer Decoder) ì²˜ìŒë¶€í„° í•™ìŠµ
	â€¢	â€œìš”ì•½/ì •ë¦¬ í¬ë§·â€ìœ¼ë¡œ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ê¹Œì§€ ì™„ì£¼í•©ë‹ˆë‹¤.

â€œë¹ˆ íŒŒì¼ì—ì„œ ì‹œì‘í•´ì„œ GPT-2ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•˜ëŠ” í•™ìŠµ ë£¨íŠ¸â€ëŠ” Karpathyì˜ build-nanogptê°€ ê°€ì¥ êµìœ¡ì ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.  ï¿¼

â¸»

0) ë‹¹ì‹ ì´ ë§Œë“¤ LLMì˜ ìµœì¢… ì¶œë ¥ í¬ë§·(ëª©í‘œ)

ëª¨ë¸ì´ ì´ëŸ° ìŠ¤íƒ€ì¼ë¡œ ë‹µí•˜ë„ë¡ ë§Œë“¤ ê²ë‹ˆë‹¤:

âœ… ìš”ì•½(í•µì‹¬ë§Œ)
âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸(ì‹¤í–‰ ìˆœì„œ / ì¤€ë¹„ë¬¼ / ì£¼ì˜ì‚¬í•­)
âœ… ì—°ë½ì²˜(ê³µê³µê¸°ê´€, ë³‘ì›, ëŒ€ì‚¬ê´€ ë“±)
âœ… ê¸´ ì„¤ëª…í˜•(ì™œ/ì–´ë–»ê²Œ/ì£¼ì˜ì ê¹Œì§€ ìì„¸íˆ)

ì˜ˆì‹œ(ëª¨ë¸ ì¶œë ¥ ëª©í‘œ):

ìš”ì•½
	â€¢	ë§ˆë‹ë¼ì—ì„œ ì—¬ê¶Œ ë¶„ì‹¤ ì‹œ: ê²½ì°° ì‹ ê³ ì„œ â†’ ëŒ€ì‚¬ê´€ ì—°ë½ â†’ ì—¬í–‰ì¦ëª…ì„œ/ì¬ë°œê¸‰ ì§„í–‰ ìˆœì„œë¡œ ì²˜ë¦¬

ì²´í¬ë¦¬ìŠ¤íŠ¸
	â€¢	ê²½ì°°ì„œ ë°©ë¬¸í•´ Police Report ë°›ê¸°
	â€¢	ì—¬ê¶Œ ì‚¬ë³¸/ì‚¬ì§„ ì¤€ë¹„
	â€¢	ëŒ€ì‚¬ê´€/ì´ì˜ì‚¬ê´€ ì•ˆë‚´ í™•ì¸
	â€¢	í•­ê³µê¶Œ ì¼ì • ì¡°ì •

ì—°ë½ì²˜(ê³µê³µì •ë³´)
	â€¢	ì£¼í•„ë¦¬í•€ ëŒ€í•œë¯¼êµ­ ëŒ€ì‚¬ê´€ TEL: â€¦
	â€¢	ê´€í•  ê²½ì°°ì„œ TEL: â€¦
	â€¢	ê¸´ê¸‰ ë³‘ì› TEL: â€¦

ìƒì„¸ ì„¤ëª…
	â€¢	ì™œ Police Reportê°€ ë¨¼ì € í•„ìš”í•œì§€â€¦
	â€¢	ëŒ€ì‚¬ê´€ ì—…ë¬´ì‹œê°„/íœ´ì¼ì— ë”°ë¥¸ í”ŒëœBâ€¦
	â€¢	ì¬ë°œê¸‰ê³¼ ì—¬í–‰ì¦ëª…ì„œ ì°¨ì´â€¦

â¸»

1) Macbook M4(MPS)ì—ì„œ í•™ìŠµ í™˜ê²½ ë§Œë“¤ê¸°

1-1. ì„¤ì¹˜

python -m venv .venv
source .venv/bin/activate

pip install torch tokenizers tqdm numpy

1-2. MPS(GPU) ì‚¬ìš© í™•ì¸

PyTorchëŠ” Macì—ì„œ MPS ë””ë°”ì´ìŠ¤ë¥¼ í†µí•´ GPU ê°€ì† í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.  ï¿¼

import torch
print(torch.backends.mps.is_available())
device = "mps" if torch.backends.mps.is_available() else "cpu"
print("device =", device)

1-3. (ì¤‘ìš”) MPS fallback ì¼œê¸°

MPSì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì—°ì‚°ì´ ë‚˜ì˜¤ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ì„œ, ì„ì‹œ í•´ê²°ë¡œ CPU fallbackì„ ì¼¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ï¿¼

í„°ë¯¸ë„ì—ì„œ:

export PYTORCH_ENABLE_MPS_FALLBACK=1


â¸»

2) í”„ë¡œì íŠ¸ í´ë”(ì´ëŒ€ë¡œ ë§Œë“¤ë©´ ë¨)

my_ph_llm/
  data/
    raw.txt                 # ì›ë³¸ txt (100MB)
    cleaned.txt             # ì •ë¦¬ëœ txt
    tokenizer.json          # ë‚´ ë°ì´í„°ë¡œ í•™ìŠµí•œ í† í¬ë‚˜ì´ì €
    train.bin               # í•™ìŠµìš© í† í° ì‹œí€€ìŠ¤
    val.bin                 # ê²€ì¦ìš© í† í° ì‹œí€€ìŠ¤
  01_clean_normalize.py
  02_train_tokenizer.py
  03_build_bin_dataset.py
  04_train_gpt_from_scratch.py
  05_generate.py


â¸»

3) ë°ì´í„° ì „ëµ: â€œìš”ì•½/ì •ë¦¬ ëŠ¥ë ¥â€ì€ ë°ì´í„° í¬ë§·ì´ ë§Œë“ ë‹¤ (í•µì‹¬)

ì²˜ìŒë¶€í„° í•™ìŠµí•˜ëŠ” LLMì€ ê¸°ë³¸ì ìœ¼ë¡œ:

â€œë‹¤ìŒ í† í° ë§ì¶”ê¸°â€ë¥¼ ì˜í•˜ê²Œ ë©ë‹ˆë‹¤.

ë”°ë¼ì„œ â€œìš”ì•½ì„ ì˜í•˜ëŠ” ëª¨ë¸â€ì„ ë§Œë“¤ë ¤ë©´
í•™ìŠµ í…ìŠ¤íŠ¸ ìì²´ì— ìš”ì•½/ì •ë¦¬ëœ ë‹µë³€ì´ ë§ì´ ì¡´ì¬í•´ì•¼ í•´ìš”.

âœ… í•´ê²°ë²•: txtë¥¼ ì•„ë˜ í˜•íƒœë¡œ â€œí•™ìŠµ ê°€ëŠ¥í•œ ë¬¸ì„œ í¬ë§·â€ìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.

â¸»

3-1) ì¶”ì²œ í•™ìŠµ í¬ë§· (ë‹¹ì‹ ì—ê²Œ ìµœì )

ë‹¹ì‹  ë°ì´í„°ê°€ â€œí•„ë¦¬í•€ êµë¯¼ ì •ë³´â€ë‹ˆê¹Œ ì•„ë˜ í…œí”Œë¦¿ì´ ìµœê³ ì…ë‹ˆë‹¤:

[DOC]
ì œëª©: (ë‚´ìš©ì˜ ì£¼ì œ)
ë³¸ë¬¸:
(ì›ë¬¸ ë‚´ìš©)
[/DOC]

[ANSWER]
ìš”ì•½:
- (í•µì‹¬ 3~7ê°œ)

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- ì¤€ë¹„ë¬¼:
  - ...
- í•´ì•¼ í•  ì¼:
  - ...
- ì£¼ì˜ì‚¬í•­:
  - ...

ì—°ë½ì²˜(ê³µê³µì •ë³´):
- ê¸°ê´€ëª…: ...
  - TEL: ...
  - ADDR: ...
  - WEB: ...

ìƒì„¸ ì„¤ëª…:
(ì—¬ê¸°ë¶€í„° ê¸¸ê²Œ ì„¤ëª…)
[/ANSWER]

ğŸ“Œ ì—¬ê¸°ì„œ í¬ì¸íŠ¸ëŠ”:
	â€¢	ëª¨ë¸ì´ â€œìš”ì•½/ì²´í¬ë¦¬ìŠ¤íŠ¸/ì—°ë½ì²˜/ì„¤ëª…â€ íŒ¨í„´ì„ ì–¸ì–´ íŒ¨í„´ìœ¼ë¡œ í†µì§¸ë¡œ í•™ìŠµí•˜ê²Œ ë¨
	â€¢	íŒŒì¸íŠœë‹ ì—†ì´ë„ ì¶œë ¥ í˜•ì‹ì´ ìŠµê´€ì²˜ëŸ¼ ê³ ì •ë©ë‹ˆë‹¤.

â¸»

4) 01_clean_normalize.py (ì „ì²˜ë¦¬ + í‘œì¤€í™”)

ê³µê³µê¸°ê´€ ì—°ë½ì²˜ëŠ” â€œê°œì¸ì •ë³´â€ë¼ê¸°ë³´ë‹¤ â€œì •ë³´ ì œê³µìš© í•µì‹¬ ë°ì´í„°â€ì´ë¯€ë¡œ
âœ… ì‚­ì œí•˜ì§€ ë§ê³  í‘œì¤€í™”í•´ì„œ ëª¨ë¸ì´ ë°°ìš°ê¸° ì‰½ê²Œ ë§Œë“œëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤.

ì˜ˆ: ì „í™”, Tel, â˜ï¸ â†’ ì „ë¶€ TEL: ë¡œ í†µì¼

# 01_clean_normalize.py
import re

IN_PATH = "data/raw.txt"
OUT_PATH = "data/cleaned.txt"

def clean_text(s: str) -> str:
    # ì œì–´ë¬¸ì ì œê±°
    s = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", " ", s)

    # ê³µë°± ì •ë¦¬
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)

    # ë¼ë²¨ í‘œì¤€í™”
    s = re.sub(r"(â˜ï¸|ì „í™”|TEL|Tel|tel)\s*[:ï¼š]?\s*", "TEL: ", s)
    s = re.sub(r"(ì£¼ì†Œ|ADDR|Addr|addr)\s*[:ï¼š]?\s*", "ADDR: ", s)
    s = re.sub(r"(í™ˆí˜ì´ì§€|ì›¹ì‚¬ì´íŠ¸|ì‚¬ì´íŠ¸|WEB|Web|web)\s*[:ï¼š]?\s*", "WEB: ", s)

    return s.strip()

with open(IN_PATH, "r", encoding="utf-8") as f:
    raw = f.read()

cleaned = clean_text(raw)

with open(OUT_PATH, "w", encoding="utf-8") as f:
    f.write(cleaned)

print("saved:", OUT_PATH, "chars:", len(cleaned))

ì‹¤í–‰:

python 01_clean_normalize.py


â¸»

5) 02_train_tokenizer.py (BPE í† í¬ë‚˜ì´ì €ë¥¼ ë‚´ ë°ì´í„°ë¡œ ì§ì ‘ í•™ìŠµ)

í† í¬ë‚˜ì´ì € í•™ìŠµì€ Hugging Face tokenizersì˜ Trainer APIë¡œ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤.  ï¿¼
(vocab_size, special_tokens ë“± ì˜µì…˜ ì„¤ëª…ì´ ë¬¸ì„œì— ë‚˜ì˜µë‹ˆë‹¤.)

# 02_train_tokenizer.py
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer

IN_PATH = "data/cleaned.txt"
OUT_PATH = "data/tokenizer.json"

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

trainer = BpeTrainer(
    vocab_size=24000,
    special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]"],
    show_progress=True,
)

tokenizer.train([IN_PATH], trainer=trainer)
tokenizer.save(OUT_PATH)

print("saved tokenizer:", OUT_PATH)

ì‹¤í–‰:

python 02_train_tokenizer.py

âœ… í•œêµ­ì–´ + â€œí•„ë¦¬í•€ ì§€ëª…/ê¸°ê´€ëª…/ìš©ì–´â€ê°€ ë§ê¸° ë•Œë¬¸ì—
ë‚´ ë°ì´í„°ë¡œ í•™ìŠµí•œ í† í¬ë‚˜ì´ì €ê°€ ë§¤ìš° ìœ ë¦¬í•©ë‹ˆë‹¤.

â¸»

6) 03_build_bin_dataset.py (í† í°í™” â†’ í•™ìŠµìš© ë°”ì´ë„ˆë¦¬ ì €ì¥)

# 03_build_bin_dataset.py
import numpy as np
from tokenizers import Tokenizer

TEXT_PATH = "data/cleaned.txt"
TOK_PATH = "data/tokenizer.json"

TRAIN_OUT = "data/train.bin"
VAL_OUT = "data/val.bin"

VAL_RATIO = 0.01

tokenizer = Tokenizer.from_file(TOK_PATH)

with open(TEXT_PATH, "r", encoding="utf-8") as f:
    text = f.read()

ids = tokenizer.encode(text).ids
arr = np.array(ids, dtype=np.uint16)

n = len(arr)
n_val = int(n * VAL_RATIO)

train = arr[:-n_val]
val = arr[-n_val:]

train.tofile(TRAIN_OUT)
val.tofile(VAL_OUT)

print("train tokens:", len(train))
print("val tokens:", len(val))

ì‹¤í–‰:

python 03_build_bin_dataset.py


â¸»

7) 04_train_gpt_from_scratch.py (ì§„ì§œ â€œì´ˆê°„ë‹¨ GPTâ€ í•™ìŠµ)

ì—¬ê¸°ì„œë¶€í„°ê°€ â€œì™„ì „í•œ LLMâ€ì…ë‹ˆë‹¤.
êµ¬ì¡°ëŠ” **Decoder-only Transformer (GPT í˜•íƒœ)**ì´ê³ , í•™ìŠµì€ next-token predictionì…ë‹ˆë‹¤.

êµìœ¡ìš©ìœ¼ë¡œëŠ” build-nanogptê°€ ë¹ˆ íŒŒì¼ì—ì„œ ì‹œì‘í•´ ì™„ì„±ê¹Œì§€ ë³´ì—¬ì¤˜ì„œ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤.  ï¿¼

7-1) ì¶”ì²œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (M4 ì„±ê³µ í™•ë¥  ìµœìƒ)

ì²˜ìŒì—” ë¬´ì¡°ê±´ ì‘ê²Œ:
	â€¢	block_size = 256
	â€¢	n_layer = 6
	â€¢	n_head = 6
	â€¢	n_embd = 384
	â€¢	batch_size = 8~16
	â€¢	steps = 10k~30k

ì´ ì„¤ì •ì´ë©´ MPSì—ì„œ â€œí•™ìŠµì´ ë˜ëŠ” ëŠë‚Œâ€ì„ ê½¤ ë¹¨ë¦¬ ë´…ë‹ˆë‹¤.

ì°¸ê³ : PyTorchì—ëŠ” MPS ë©”ëª¨ë¦¬/ì›Œí„°ë§ˆí¬ ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ ë¬¸ì„œë„ ìˆìŠµë‹ˆë‹¤(ë„ˆë¬´ ìì£¼ OOM ë‚˜ë©´ ì°¸ê³ ).  ï¿¼

â¸»

8) 05_generate.py (ìš”ì•½+ì²´í¬ë¦¬ìŠ¤íŠ¸+ì—°ë½ì²˜+ìƒì„¸ì„¤ëª… ìƒì„±)

ìƒì„±ì€ â€œìš”ì•½í˜• LLMâ€ì—ì„œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì´ ê±°ì˜ ë°˜ ì´ìƒì…ë‹ˆë‹¤.

âœ… ì¶”ì²œ í”„ë¡¬í”„íŠ¸(ê³ ì •):

[QUESTION]
í•„ë¦¬í•€ì—ì„œ í•œêµ­ì¸ì´ ë³‘ì› ê°€ì•¼ í•  ë•Œ ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ì¶”ì²œ ê¸°ì¤€ì„ ìì„¸íˆ ì •ë¦¬í•´ì¤˜.
[/QUESTION]

[ANSWER]
ìš”ì•½:
-

ì´ë ‡ê²Œ [ANSWER] ìš”ì•½:ê¹Œì§€ ì¨ì£¼ë©´
ëª¨ë¸ì€ ë‹¤ìŒ í† í°ì„ ì´ì–´ ì“°ë©´ì„œ:
	â€¢	ìš”ì•½ bullet
	â€¢	ì²´í¬ë¦¬ìŠ¤íŠ¸
	â€¢	ì—°ë½ì²˜ ì„¹ì…˜
	â€¢	ìƒì„¸ì„¤ëª…

ì„ â€œìŠµê´€ì²˜ëŸ¼â€ ì´ì–´ ì‘ì„±í•˜ê²Œ ë©ë‹ˆë‹¤.

â¸»

9) ê³µë¶€í•  ë•Œ ê¼­ ì´í•´í•´ì•¼ í•˜ëŠ” í•µì‹¬ ê°œë… 9ê°œ (ì§„ì§œ ì¤‘ìš”)
	1.	Tokenizer

	â€¢	â€œí…ìŠ¤íŠ¸ â†’ ì •ìˆ˜ í† í°â€ ë³€í™˜ê¸°
	â€¢	vocab_sizeê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ í•œêµ­ì–´ê°€ ê¹¨ì§€ê³ , ë„ˆë¬´ í¬ë©´ í•™ìŠµì´ ì–´ë ¤ì›Œì§

	2.	Embedding

	â€¢	í† í° IDë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” í‘œ

	3.	Positional Encoding/Embedding

	â€¢	í† í° ìˆœì„œë¥¼ ëª¨ë¸ì´ ì´í•´í•˜ê²Œ ë§Œë“œëŠ” ìš”ì†Œ(GPTëŠ” ìœ„ì¹˜ ì„ë² ë”© ìì£¼ ì‚¬ìš©)

	4.	Self-Attention

	â€¢	ë¬¸ì¥ ì•ˆì—ì„œ â€œì–´ë–¤ ë‹¨ì–´ê°€ ì¤‘ìš”í•œì§€â€ë¥¼ ì°¸ì¡°í•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

	5.	Feed Forward

	â€¢	attention ë‹¤ìŒì— ì •ë³´ë¥¼ ë¹„ì„ í˜• ë³€í™˜í•´ì„œ í‘œí˜„ë ¥ì„ ëŠ˜ë¦¼

	6.	Residual + LayerNorm

	â€¢	ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ê²Œ í•´ì£¼ëŠ” ì¥ì¹˜

	7.	Next-token prediction

	â€¢	GPTì˜ í•™ìŠµ ëª©í‘œ: â€œë‹¤ìŒ í† í° ë§ì¶”ê¸°â€

	8.	Sampling

	â€¢	temperature / top-k / top-pë¡œ ì¶œë ¥ ë‹¤ì–‘ì„± ì¡°ì ˆ

	9.	ë°ì´í„° í¬ë§·ì´ ê³§ ëª¨ë¸ ëŠ¥ë ¥

	â€¢	ìš”ì•½í˜•ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©´ ìš”ì•½í˜•ìœ¼ë¡œ ë§í•¨
	â€¢	ì²´í¬ë¦¬ìŠ¤íŠ¸í˜•ì´ë©´ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë½‘ìŒ

â¸»

10) ë‹¹ì‹ ì˜ ë°ì´í„°(í•„ë¦¬í•€ êµë¯¼ì •ë³´)ì—ì„œ íŠ¹íˆ ì¶”ì²œí•˜ëŠ” â€œí•™ìŠµ ë°ì´í„° ê°•í™”â€ íŒ

âœ… â€œí•œ ë¬¸ì„œë‹¹ 1ê°œì˜ ìš”ì•½ ë‹µë³€â€ì„ ë§Œë“¤ì–´ë¼

raw.txtë¥¼ ê·¸ëŒ€ë¡œ í•™ìŠµì‹œí‚¤ë©´ â€œí…ìŠ¤íŠ¸ ì´ì–´ì“°ê¸°â€ë§Œ ì˜í•©ë‹ˆë‹¤.

ëŒ€ì‹  ì•„ë˜ì²˜ëŸ¼ â€œì§ˆë¬¸â†’ë‹µë³€â€ êµ¬ì¡° ìƒ˜í”Œì„ ë§ì´ ë§Œë“¤ë©´ ì¢‹ì•„ìš”:
	â€¢	[QUESTION] í•„ë¦¬í•€ ACR I-Card ì—°ì¥ ë°©ë²• ì •ë¦¬í•´ì¤˜
	â€¢	[QUESTION] ë§ˆë‹ë¼ì—ì„œ ê²½ì°° ì‹ ê³ ì„œ ë°›ëŠ” ë²•
	â€¢	[QUESTION] êµë¯¼ì´ ìì£¼ ì“°ëŠ” ê¸´ê¸‰ ì—°ë½ì²˜ ëª¨ìŒ

ì´ê±¸ ìˆ˜ì²œ ê°œ ë§Œë“¤ì–´ë„ ë˜ê³ , ìë™ìœ¼ë¡œ ìƒì„±í•´ë„ ë©ë‹ˆë‹¤(ê·œì¹™ ê¸°ë°˜).

âœ… ì—°ë½ì²˜ëŠ” â€œì •í™•í•œ í¬ë§·â€ìœ¼ë¡œ ê³ ì •

ì˜ˆ:

ì—°ë½ì²˜(ê³µê³µì •ë³´):
- ì£¼í•„ë¦¬í•€ ëŒ€í•œë¯¼êµ­ ëŒ€ì‚¬ê´€
  - TEL: ...
  - ADDR: ...
  - WEB: ...

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ì¶œë ¥ë„ ë¹„ìŠ·í•˜ê²Œ í•˜ë ¤ í•©ë‹ˆë‹¤.

â¸»

11) ë‹¤ìŒ ë‹¨ê³„: ì œê°€ â€œí•™ìŠµ ì½”ë“œ ì „ì²´ ì™„ì„±ë³¸â€ì„ ë“œë¦´ê¹Œìš”?

ë‹¹ì‹ ì´ ì›í•œ ê±´ â€œìµœëŒ€í•œ ìƒì„¸ ì •ë³´â€ + â€œì™„ì „ ì´ˆê°„ë‹¨ LLM ì™„ì„±â€ì´ë‹ˆê¹Œ,

ì›í•˜ì‹œë©´ ë‹¤ìŒ ë‹µë³€ì—ì„œ ì œê°€:

âœ… 04_train_gpt_from_scratch.py ì „ì²´ ì½”ë“œ(í•™ìŠµ/ì²´í¬í¬ì¸íŠ¸/ì¬ê°œ í¬í•¨)
âœ… 05_generate.py ì „ì²´ ì½”ë“œ(temperature/top-k í¬í•¨)
âœ… ì‹¤í–‰ ì»¤ë§¨ë“œ + ì˜ˆìƒ ì†Œìš”ì‹œê°„ + íŠœë‹ í¬ì¸íŠ¸

ë¥¼ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ë©´ ëŒì•„ê°€ê²Œ ì™„ì„±ë³¸ìœ¼ë¡œ ë“œë¦´ê²Œìš”.

â¸»

ì¢‹ìŠµë‹ˆë‹¤ ğŸ‘
ì•„ë˜ëŠ” â€œraw.txt(í•œ íŒŒì¼ì— ì¤„ì¤„ì´ ì„ì„)â€ â†’ ì „ì²˜ë¦¬/ë¶„í•  â†’ í† í¬ë‚˜ì´ì €(BPE) í•™ìŠµ â†’ GPT(Decoder Transformer) from scratch í•™ìŠµ â†’ ìš”ì•½/ì •ë¦¬+ìƒì„¸ì„¤ëª… ìƒì„±ê¹Œì§€ ë˜ëŠ” ì™„ì„±ë³¸ ì½”ë“œì…ë‹ˆë‹¤.

Macbook M4(MPS)ì—ì„œ ëŒì•„ê°€ë„ë¡ ë§Œë“¤ì—ˆê³ , PyTorch MPSì—ì„œ ë¯¸ì§€ì› ì—°ì‚°ì´ ë‚˜ì˜¤ë©´ CPU fallbackì„ ì¼¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.  ï¿¼
í† í¬ë‚˜ì´ì €ëŠ” Hugging Face tokenizersì˜ BPE Trainer ê¸°ë°˜ì…ë‹ˆë‹¤.  ï¿¼
(êµ¬í˜„ ìŠ¤íƒ€ì¼ì€ Karpathyì˜ from-scratch GPT êµ¬í˜„ íë¦„ì„ ì°¸ê³ í•˜ë©´ í•™ìŠµì— ë„ì›€ë©ë‹ˆë‹¤.  ï¿¼)

â¸»

âœ… 0) í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°

my_ph_llm/
  data/
    raw.txt
  01_prepare_samples.py
  02_train_tokenizer.py
  03_build_bin_dataset.py
  04_train_gpt_from_scratch.py
  05_generate.py


â¸»

âœ… 1) ì„¤ì¹˜ & ì‹¤í–‰ ì¤€ë¹„

cd my_ph_llm
python -m venv .venv
source .venv/bin/activate

pip install torch tokenizers tqdm numpy

(ê¶Œì¥) MPS fallback ì¼œê¸°:  ï¿¼

export PYTORCH_ENABLE_MPS_FALLBACK=1


â¸»

âœ… 2) 01_prepare_samples.py

raw.txtê°€ â€œì¤„ì¤„ì´ ì„ì¸ í˜•íƒœâ€ì´ë¯€ë¡œ â†’ ìë™ìœ¼ë¡œ ë¬¸ì„œ ë©ì–´ë¦¬ë¡œ ìª¼ê°œê³  â†’ ìš”ì•½/ì²´í¬ë¦¬ìŠ¤íŠ¸/ì—°ë½ì²˜/ìƒì„¸ì„¤ëª… í¬ë§·ìœ¼ë¡œ â€œí•™ìŠµ í…ìŠ¤íŠ¸â€ë¥¼ ë§Œë“­ë‹ˆë‹¤.

from-scratch í•™ìŠµì—ì„œëŠ” â€œìš”ì•½ ëŠ¥ë ¥â€ì´ ëª¨ë¸ì— ì €ì ˆë¡œ ìƒê¸°ì§€ ì•Šê³ , ì´ëŸ° ì¶œë ¥ í¬ë§·ì„ í•™ìŠµ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤.

# 01_prepare_samples.py
import re
import random

RAW_PATH = "data/raw.txt"
OUT_PATH = "data/samples.txt"

random.seed(42)

def normalize_text(s: str) -> str:
    # ì œì–´ë¬¸ì ì œê±°
    s = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", " ", s)
    # ê³µë°± ì •ë¦¬
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)

    # ë¼ë²¨ í‘œì¤€í™”(ì—°ë½ì²˜ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ì¢‹ê²Œ)
    s = re.sub(r"(â˜ï¸|ì „í™”|TEL|Tel|tel)\s*[:ï¼š]?\s*", "TEL: ", s)
    s = re.sub(r"(ì£¼ì†Œ|ADDR|Addr|addr)\s*[:ï¼š]?\s*", "ADDR: ", s)
    s = re.sub(r"(í™ˆí˜ì´ì§€|ì›¹ì‚¬ì´íŠ¸|ì‚¬ì´íŠ¸|WEB|Web|web)\s*[:ï¼š]?\s*", "WEB: ", s)

    return s.strip()

def split_into_chunks(text: str, min_len=600, max_len=1800):
    """
    ì¤„ì¤„ì´ ì„ì¸ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë©ì–´ë¦¬ë¡œ ìª¼ê°œëŠ” ì´ˆê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±.
    - ë¹ˆ ì¤„ 2ê°œ ì´ìƒì„ ê²½ê³„ë¡œ 1ì°¨ ë¶„ë¦¬
    - ë„ˆë¬´ ì§§ì€ ê±´ í•©ì¹˜ê³ , ë„ˆë¬´ ê¸´ ê±´ ì˜ë¼ì„œ ë‚˜ëˆ”
    """
    parts = re.split(r"\n\s*\n", text)
    parts = [p.strip() for p in parts if p.strip()]

    chunks = []
    buf = ""

    for p in parts:
        if len(buf) < min_len:
            buf = (buf + "\n\n" + p).strip() if buf else p
        else:
            chunks.append(buf)
            buf = p

    if buf:
        chunks.append(buf)

    # ë„ˆë¬´ ê¸´ chunkëŠ” ê°•ì œë¡œ ìª¼ê°¬
    final = []
    for c in chunks:
        if len(c) <= max_len:
            final.append(c)
        else:
            for i in range(0, len(c), max_len):
                final.append(c[i:i+max_len].strip())
    return [c for c in final if len(c) >= min_len]

QUESTION_BANK = [
    "ì´ ë‚´ìš©ì„ êµë¯¼ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì¤˜.",
    "í•„ë¦¬í•€ì—ì„œ í•œêµ­ì¸ ì…ì¥ì—ì„œ í•µì‹¬ë§Œ ì •ë¦¬í•´ì¤˜.",
    "ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ì£¼ì˜ì‚¬í•­ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.",
    "ì—°ë½ì²˜ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì •ë¦¬í•´ì¤˜.",
    "ì´ˆë³´ êµë¯¼ì„ ìœ„í•œ ì•ˆë‚´ë¬¸ì²˜ëŸ¼ ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜.",
]

def build_training_sample(doc: str) -> str:
    # ì—°ë½ì²˜ í›„ë³´ ì¶”ì¶œ(ë„ˆë¬´ ì •êµí•  í•„ìš” ì—†ìŒ)
    tels = re.findall(r"TEL:\s*([0-9+\-\s()]{6,})", doc)
    webs = re.findall(r"WEB:\s*(\S+)", doc)

    # â€œìš”ì•½/ì²´í¬ë¦¬ìŠ¤íŠ¸/ì—°ë½ì²˜/ìƒì„¸ì„¤ëª…â€ í¬ë§·ì„ ê°•ì œë¡œ í•™ìŠµì‹œí‚¤ëŠ” í…ìŠ¤íŠ¸
    q = random.choice(QUESTION_BANK)

    # ì•„ì£¼ ê°„ë‹¨í•œ â€œìš”ì•½ ë¬¸ì¥â€ íŒíŠ¸(í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ì¼ë¶€ëŠ” ë‚¨ê¹€)
    # â€» from scratchë¼ì„œ ì—¬ê¸°ì„œ ìš”ì•½ì„ ì™„ë²½íˆ ë§Œë“¤ í•„ìš”ëŠ” ì—†ê³ ,
    #    "í˜•ì‹"ì„ ê°•í•˜ê²Œ ì£¼ì…í•˜ëŠ” ê²Œ ëª©ì .
    hint = doc[:200].replace("\n", " ").strip()

    contacts = ""
    if tels or webs:
        contacts += "ì—°ë½ì²˜(ê³µê³µì •ë³´):\n"
        if tels:
            for t in list(dict.fromkeys(tels))[:3]:
                contacts += f"- TEL: {t.strip()}\n"
        if webs:
            for w in list(dict.fromkeys(webs))[:2]:
                contacts += f"- WEB: {w.strip()}\n"
    else:
        contacts = "ì—°ë½ì²˜(ê³µê³µì •ë³´):\n- (ë¬¸ì„œì— ëª…ì‹œëœ ì—°ë½ì²˜ê°€ ì—†ê±°ë‚˜ í™•ì¸ í•„ìš”)\n"

    sample = f"""[QUESTION]
{q}
[/QUESTION]

[DOC]
{doc}
[/DOC]

[ANSWER]
ìš”ì•½:
- {hint} ...

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- í•´ì•¼ í•  ì¼:
  - (1) í•µì‹¬ í–‰ë™ 1
  - (2) í•µì‹¬ í–‰ë™ 2
- ì¤€ë¹„ë¬¼:
  - (1) í•„ìš”í•œ ì„œë¥˜/ì •ë³´
- ì£¼ì˜ì‚¬í•­:
  - (1) ì‹¤ìˆ˜í•˜ê¸° ì‰¬ìš´ ì 

{contacts}
ìƒì„¸ ì„¤ëª…:
ì´ ë¬¸ì„œëŠ” í•„ë¦¬í•€(ì¬í•„ êµë¯¼/ë™í¬) ìƒí™œ ì •ë³´ ê´€ì ì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
ìœ„ ìš”ì•½ê³¼ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ ìƒí™©ì— ë§ê²Œ ë‹¨ê³„ë³„ë¡œ ì§„í–‰í•˜ì„¸ìš”.
ë¬¸ì„œì— í¬í•¨ëœ ê·œì •/ë¹„ìš©/ì—…ë¬´ì‹œê°„/ê´€í• ì§€ì—­ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ìµœì‹  ì •ë³´ë¥¼ ì¬í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
[/ANSWER]

"""
    return sample

def main():
    with open(RAW_PATH, "r", encoding="utf-8") as f:
        raw = f.read()

    raw = normalize_text(raw)
    chunks = split_into_chunks(raw)

    # ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ(í•™ìŠµ ì†ë„ ìœ„í•´)
    # 100MBë©´ chunkê°€ ë§ì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ìš°ì„  30kê°œ ì´í•˜ë¥¼ ì¶”ì²œ
    if len(chunks) > 30000:
        chunks = chunks[:30000]

    print("chunks:", len(chunks))

    with open(OUT_PATH, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(build_training_sample(c))

    print("saved:", OUT_PATH)

if __name__ == "__main__":
    main()

ì‹¤í–‰:

python 01_prepare_samples.py

âœ… ê²°ê³¼: data/samples.txt (ìš”ì•½/ì²´í¬ë¦¬ìŠ¤íŠ¸/ì—°ë½ì²˜/ìƒì„¸ì„¤ëª… í•™ìŠµìš©)

â¸»

âœ… 3) 02_train_tokenizer.py (ë‚´ ë°ì´í„°ë¡œ BPE í† í¬ë‚˜ì´ì € í•™ìŠµ)

BPE Trainerì˜ í•µì‹¬ ì˜µì…˜(vocab_size, special_tokens ë“±)ì€ Hugging Face ë¬¸ì„œì— ìˆìŠµë‹ˆë‹¤.  ï¿¼

# 02_train_tokenizer.py
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer

IN_PATH = "data/samples.txt"
OUT_PATH = "data/tokenizer.json"

VOCAB_SIZE = 24000

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

trainer = BpeTrainer(
    vocab_size=VOCAB_SIZE,
    special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]"],
    show_progress=True,
)

tokenizer.train([IN_PATH], trainer=trainer)
tokenizer.save(OUT_PATH)

print("saved tokenizer:", OUT_PATH)

ì‹¤í–‰:

python 02_train_tokenizer.py


â¸»

âœ… 4) 03_build_bin_dataset.py (í† í°í™” â†’ train/val ë°”ì´ë„ˆë¦¬)

# 03_build_bin_dataset.py
import numpy as np
from tokenizers import Tokenizer

TEXT_PATH = "data/samples.txt"
TOK_PATH = "data/tokenizer.json"

TRAIN_OUT = "data/train.bin"
VAL_OUT = "data/val.bin"

VAL_RATIO = 0.01

tokenizer = Tokenizer.from_file(TOK_PATH)

with open(TEXT_PATH, "r", encoding="utf-8") as f:
    text = f.read()

ids = tokenizer.encode(text).ids
arr = np.array(ids, dtype=np.uint16)

n = len(arr)
n_val = int(n * VAL_RATIO)

train = arr[:-n_val]
val = arr[-n_val:]

train.tofile(TRAIN_OUT)
val.tofile(VAL_OUT)

print("train tokens:", len(train), "->", TRAIN_OUT)
print("val tokens:", len(val), "->", VAL_OUT)

ì‹¤í–‰:

python 03_build_bin_dataset.py


â¸»

âœ… 5) 04_train_gpt_from_scratch.py (í•µì‹¬: GPT from scratch í•™ìŠµ ì½”ë“œ)

ì•„ë˜ëŠ” ì™„ì „í•œ ì´ˆê°„ë‹¨ GPT í•™ìŠµ ì½”ë“œì…ë‹ˆë‹¤.
	â€¢	MPS ìë™ ì‚¬ìš©
	â€¢	ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ì¬ê°œ
	â€¢	í•™ìŠµ ì¤‘ê°„ generate ìƒ˜í”Œ ì¶œë ¥

# 04_train_gpt_from_scratch.py
import os
import math
import time
import numpy as np
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
from tokenizers import Tokenizer
from tqdm import tqdm

# -----------------------------
# Config
# -----------------------------
@dataclass
class CFG:
    train_bin: str = "data/train.bin"
    val_bin: str = "data/val.bin"
    tok_path: str = "data/tokenizer.json"
    out_dir: str = "checkpoints"

    # model
    block_size: int = 256
    n_layer: int = 6
    n_head: int = 6
    n_embd: int = 384
    dropout: float = 0.1

    # train
    batch_size: int = 16
    lr: float = 3e-4
    max_steps: int = 20000
    eval_interval: int = 500
    eval_iters: int = 100
    grad_clip: float = 1.0

    # sampling (debug)
    sample_every_eval: bool = True
    sample_max_new_tokens: int = 250
    temperature: float = 0.9
    top_k: int = 50

cfg = CFG()

os.makedirs(cfg.out_dir, exist_ok=True)

# -----------------------------
# Device
# -----------------------------
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

device = get_device()
print("device =", device)

# -----------------------------
# Tokenizer
# -----------------------------
tokenizer = Tokenizer.from_file(cfg.tok_path)
vocab_size = tokenizer.get_vocab_size()
print("vocab_size =", vocab_size)

# -----------------------------
# Data loader (bin)
# -----------------------------
train_data = np.memmap(cfg.train_bin, dtype=np.uint16, mode="r")
val_data = np.memmap(cfg.val_bin, dtype=np.uint16, mode="r")

def get_batch(split: str):
    data = train_data if split == "train" else val_data
    ix = torch.randint(len(data) - cfg.block_size - 1, (cfg.batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+cfg.block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+cfg.block_size]).astype(np.int64)) for i in ix])
    return x.to(device), y.to(device)

# -----------------------------
# Model (GPT)
# -----------------------------
class CausalSelfAttention(nn.Module):
    def __init__(self, n_embd, n_head, dropout, block_size):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_head = n_head
        self.head_dim = n_embd // n_head

        self.qkv = nn.Linear(n_embd, 3 * n_embd)
        self.proj = nn.Linear(n_embd, n_embd)

        self.attn_drop = nn.Dropout(dropout)
        self.resid_drop = nn.Dropout(dropout)

        # causal mask
        self.register_buffer("bias", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))

    def forward(self, x):
        B, T, C = x.size()
        qkv = self.qkv(x)  # (B, T, 3C)
        q, k, v = qkv.split(C, dim=2)

        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, nh, T, T)
        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)

        y = att @ v  # (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.resid_drop(self.proj(y))
        return y

class MLP(nn.Module):
    def __init__(self, n_embd, dropout):
        super().__init__()
        self.fc = nn.Linear(n_embd, 4 * n_embd)
        self.proj = nn.Linear(4 * n_embd, n_embd)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc(x)
        x = F.gelu(x)
        x = self.proj(x)
        x = self.drop(x)
        return x

class Block(nn.Module):
    def __init__(self, n_embd, n_head, dropout, block_size):
        super().__init__()
        self.ln1 = nn.LayerNorm(n_embd)
        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size)
        self.ln2 = nn.LayerNorm(n_embd)
        self.mlp = MLP(n_embd, dropout)

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class GPT(nn.Module):
    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout):
        super().__init__()
        self.block_size = block_size
        self.tok_emb = nn.Embedding(vocab_size, n_embd)
        self.pos_emb = nn.Embedding(block_size, n_embd)
        self.drop = nn.Dropout(dropout)

        self.blocks = nn.ModuleList([
            Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)
        ])
        self.ln_f = nn.LayerNorm(n_embd)
        self.head = nn.Linear(n_embd, vocab_size, bias=False)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                torch.nn.init.zeros_(module.bias)

    def forward(self, idx, targets=None):
        B, T = idx.size()
        assert T <= self.block_size

        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)
        x = self.tok_emb(idx) + self.pos_emb(pos)
        x = self.drop(x)

        for b in self.blocks:
            x = b(x)

        x = self.ln_f(x)
        logits = self.head(x)  # (B, T, vocab)

        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return logits, loss

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -self.block_size:]
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :] / max(temperature, 1e-6)

            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                logits[logits < v[:, [-1]]] = float("-inf")

            probs = F.softmax(logits, dim=-1)
            next_id = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_id), dim=1)
        return idx

model = GPT(
    vocab_size=vocab_size,
    block_size=cfg.block_size,
    n_layer=cfg.n_layer,
    n_head=cfg.n_head,
    n_embd=cfg.n_embd,
    dropout=cfg.dropout,
).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)

# -----------------------------
# Checkpoint load/save
# -----------------------------
ckpt_path = os.path.join(cfg.out_dir, "ckpt.pt")
start_step = 0

if os.path.exists(ckpt_path):
    print("loading checkpoint:", ckpt_path)
    ckpt = torch.load(ckpt_path, map_location="cpu")
    model.load_state_dict(ckpt["model"])
    optimizer.load_state_dict(ckpt["optim"])
    start_step = ckpt.get("step", 0)
    print("resume from step", start_step)

@torch.no_grad()
def estimate_loss():
    model.eval()
    out = {}
    for split in ["train", "val"]:
        losses = torch.zeros(cfg.eval_iters)
        for k in range(cfg.eval_iters):
            x, y = get_batch(split)
            _, loss = model(x, y)
            losses[k] = loss.item()
        out[split] = losses.mean().item()
    model.train()
    return out

def decode_ids(ids):
    return tokenizer.decode(ids)

def quick_sample():
    prompt = """[QUESTION]
í•„ë¦¬í•€ì—ì„œ í•œêµ­ì¸ì´ ë³‘ì› ê°€ì•¼ í•  ë•Œ ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ì£¼ì˜ì‚¬í•­ì„ ìƒì„¸íˆ ì •ë¦¬í•´ì¤˜.
[/QUESTION]

[ANSWER]
ìš”ì•½:
-"""
    enc = tokenizer.encode(prompt).ids
    x = torch.tensor(enc, dtype=torch.long, device=device)[None, :]
    y = model.generate(
        x,
        max_new_tokens=cfg.sample_max_new_tokens,
        temperature=cfg.temperature,
        top_k=cfg.top_k,
    )
    out = decode_ids(y[0].tolist())
    return out

print("start training...")
t0 = time.time()

pbar = tqdm(range(start_step, cfg.max_steps))
for step in pbar:
    x, y = get_batch("train")
    logits, loss = model(x, y)

    optimizer.zero_grad(set_to_none=True)
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)
    optimizer.step()

    if step % 50 == 0:
        pbar.set_description(f"step {step} loss {loss.item():.4f}")

    if step > 0 and step % cfg.eval_interval == 0:
        losses = estimate_loss()
        print(f"\nstep {step} train_loss={losses['train']:.4f} val_loss={losses['val']:.4f}")

        # save ckpt
        torch.save({
            "model": model.state_dict(),
            "optim": optimizer.state_dict(),
            "step": step,
            "cfg": cfg.__dict__,
        }, ckpt_path)
        print("saved:", ckpt_path)

        if cfg.sample_every_eval:
            print("\n--- sample ---")
            print(quick_sample()[:2500])
            print("--------------\n")

print("done. time:", time.time() - t0)

í•™ìŠµ ì‹¤í–‰:

python 04_train_gpt_from_scratch.py


â¸»

âœ… 6) 05_generate.py (í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒì„±)

# 05_generate.py
import os
import torch
from tokenizers import Tokenizer

# ë°˜ë“œì‹œ í•™ìŠµ ì½”ë“œì˜ ì„¤ì •ê³¼ ë™ì¼í•´ì•¼ í•¨
BLOCK_SIZE = 256
N_LAYER = 6
N_HEAD = 6
N_EMBD = 384
DROPOUT = 0.1

class CausalSelfAttention(torch.nn.Module):
    def __init__(self, n_embd, n_head, dropout, block_size):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_head = n_head
        self.head_dim = n_embd // n_head

        self.qkv = torch.nn.Linear(n_embd, 3 * n_embd)
        self.proj = torch.nn.Linear(n_embd, n_embd)

        self.attn_drop = torch.nn.Dropout(dropout)
        self.resid_drop = torch.nn.Dropout(dropout)

        self.register_buffer("bias", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))

    def forward(self, x):
        import math
        import torch.nn.functional as F

        B, T, C = x.size()
        qkv = self.qkv(x)
        q, k, v = qkv.split(C, dim=2)

        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)

        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.resid_drop(self.proj(y))
        return y

class MLP(torch.nn.Module):
    def __init__(self, n_embd, dropout):
        super().__init__()
        self.fc = torch.nn.Linear(n_embd, 4 * n_embd)
        self.proj = torch.nn.Linear(4 * n_embd, n_embd)
        self.drop = torch.nn.Dropout(dropout)

    def forward(self, x):
        import torch.nn.functional as F
        return self.drop(self.proj(F.gelu(self.fc(x))))

class Block(torch.nn.Module):
    def __init__(self, n_embd, n_head, dropout, block_size):
        super().__init__()
        self.ln1 = torch.nn.LayerNorm(n_embd)
        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size)
        self.ln2 = torch.nn.LayerNorm(n_embd)
        self.mlp = MLP(n_embd, dropout)

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class GPT(torch.nn.Module):
    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout):
        super().__init__()
        self.block_size = block_size
        self.tok_emb = torch.nn.Embedding(vocab_size, n_embd)
        self.pos_emb = torch.nn.Embedding(block_size, n_embd)
        self.drop = torch.nn.Dropout(dropout)

        self.blocks = torch.nn.ModuleList([
            Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)
        ])
        self.ln_f = torch.nn.LayerNorm(n_embd)
        self.head = torch.nn.Linear(n_embd, vocab_size, bias=False)

    def forward(self, idx):
        B, T = idx.size()
        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)
        x = self.tok_emb(idx) + self.pos_emb(pos)
        x = self.drop(x)

        for b in self.blocks:
            x = b(x)

        x = self.ln_f(x)
        logits = self.head(x)
        return logits

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        import torch.nn.functional as F

        for _ in range(max_new_tokens):
            idx_cond = idx[:, -self.block_size:]
            logits = self(idx_cond)
            logits = logits[:, -1, :] / max(temperature, 1e-6)

            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                logits[logits < v[:, [-1]]] = float("-inf")

            probs = F.softmax(logits, dim=-1)
            next_id = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_id), dim=1)
        return idx

def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

device = get_device()
print("device =", device)

tokenizer = Tokenizer.from_file("data/tokenizer.json")
vocab_size = tokenizer.get_vocab_size()

model = GPT(
    vocab_size=vocab_size,
    block_size=BLOCK_SIZE,
    n_layer=N_LAYER,
    n_head=N_HEAD,
    n_embd=N_EMBD,
    dropout=DROPOUT,
).to(device)

ckpt = torch.load("checkpoints/ckpt.pt", map_location="cpu")
model.load_state_dict(ckpt["model"])
model.eval()

prompt = """[QUESTION]
í•„ë¦¬í•€ì—ì„œ í•œêµ­ì¸ì´ ê²½ì°°ì„œë¥¼ ê°€ì•¼ í•˜ëŠ” ëŒ€í‘œì ì¸ ìƒí™©ê³¼ ì ˆì°¨ë¥¼ ìƒì„¸íˆ ì •ë¦¬í•´ì¤˜.
[/QUESTION]

[ANSWER]
ìš”ì•½:
-"""

x = torch.tensor(tokenizer.encode(prompt).ids, dtype=torch.long, device=device)[None, :]
y = model.generate(x, max_new_tokens=400, temperature=0.9, top_k=50)

print(tokenizer.decode(y[0].tolist())[:4000])

ì‹¤í–‰:

python 05_generate.py


â¸»

âœ… â€œí•œ íŒŒì¼ì— ì¤„ì¤„ì´ ì„ì¸ raw.txtâ€ì—ì„œ ê²°ê³¼ë¥¼ ë” ì¢‹ê²Œ ë§Œë“œëŠ” íŒ (ì¤‘ìš”)

1) í•™ìŠµ í’ˆì§ˆì´ í™• ì¢‹ì•„ì§€ëŠ” í•µì‹¬: â€œìƒ˜í”Œ ìˆ˜ ëŠ˜ë¦¬ê¸°â€

01_prepare_samples.pyì—ì„œ chunkë¥¼ ë§ì´ ë§Œë“¤ìˆ˜ë¡ ì¢‹ì•„ìš”.
	â€¢	3,000ê°œ: â€œë˜ëŠ” ëŠë‚Œâ€
	â€¢	10,000ê°œ: â€œí˜•ì‹ì´ ì•ˆì •â€
	â€¢	30,000ê°œ: â€œìš”ì•½/ì •ë¦¬ ìŠµê´€ ê°•í•´ì§â€

2) â€œì—°ë½ì²˜â€ëŠ” ì‚­ì œí•˜ì§€ ë§ê³  í¬ë§· ê³ ì •(ì´ê²Œ í•™ìŠµì— ìœ ë¦¬)

TEL:, ADDR:, WEB: ê°™ì€ ë¼ë²¨ì´ ê³„ì† ë°˜ë³µë˜ë©´
ëª¨ë¸ì´ ìë™ìœ¼ë¡œ â€œì—°ë½ì²˜ ì„¹ì…˜â€ì„ ë§Œë“¤ê¸° ì‹œì‘í•©ë‹ˆë‹¤.

3) ëª¨ë¸ì´ ë„ˆë¬´ í—›ì†Œë¦¬í•˜ë©´
	â€¢	temperatureë¥¼ 0.7ë¡œ ë‚®ì¶”ê¸°
	â€¢	top_kë¥¼ 20ìœ¼ë¡œ ì¤„ì´ê¸°
â†’ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

â¸»

âœ… ì§€ê¸ˆ ë°”ë¡œ ì‹¤í–‰ ìˆœì„œ (ì •ë¦¬)

# 1) í•™ìŠµ ìƒ˜í”Œ ìƒì„±
python 01_prepare_samples.py

# 2) í† í¬ë‚˜ì´ì € í•™ìŠµ
python 02_train_tokenizer.py

# 3) ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì…‹ ìƒì„±
python 03_build_bin_dataset.py

# 4) LLM í•™ìŠµ
python 04_train_gpt_from_scratch.py

# 5) ìƒì„± í…ŒìŠ¤íŠ¸
python 05_generate.py


